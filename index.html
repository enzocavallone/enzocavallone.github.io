<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
<meta property="og:type" content="website">
<meta property="og:title" content="喵咪妈咪哄">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="喵咪妈咪哄">
<meta property="og:description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="nekkoya">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>喵咪妈咪哄</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">喵咪妈咪哄</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录生活&学习交流</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">6</span></a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/05/%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nekkoya">
      <meta itemprop="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵咪妈咪哄">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/05/%E9%A1%B9%E7%9B%AE%E8%83%8C%E6%99%AF-1/" class="post-title-link" itemprop="url">项目背景</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-05 14:01:15 / 修改时间：14:01:34" itemprop="dateCreated datePublished" datetime="2022-10-05T14:01:15+08:00">2022-10-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>2011 年，腾讯集团副总裁程武首先提出 “泛娱乐”的概念，人们认为依托于互联网传播技术，以 IP 明星的打造和运营为特点的新经济形态在未来将有巨大发展空间。进入2018年，盲盒，成为这一预言下的火热业态。该如何理解盲盒经济，盲盒经济又经历了怎样的发展历程呢？<br>\1. 盲盒经济定义及其发展史<br>1.1盲盒是什么？<br>（1）一般定义。盲盒指一类通常不在外包装注明内含物品，只有买后拆开才能知道内含物件一种盒子商品。通过具有猜测性的 “盲”，吸引了无数消费者持续消费。简单举例， 如玩偶类盲盒，就是外包装没有明确提醒的装有不同玩偶的纸盒，只有在购买完打开后才能知道里面具体玩偶款式，一般会区分不同系列进行售卖。因此 ，也可以将盲盒理解为一种以“概率机制产品”为特色的模糊包装销售模式。以盲盒这类商品为基础的经济被人们称为“盲盒经济”。<br>（2）盲盒的归属范畴。学术领域，人们一般将“盲盒”这类新产品归为“惊喜经济”类型，也有人将其归为“潮玩经济”或“等待经济”范畴。（“惊喜经济”指能为消费者创造惊喜心理满足为主要业态的一类商业形态，其产品形态主要有竞技抽奖、彩票、有奖挑战、娃娃机、盲盒等。“潮玩经济”则是指以具有IP属性的潮流消费品或服务为主要业态的一类商业形态，其产品形态主要以文化IP产品、玩具等的消费为主，有很强的的社交属性；“等待经济”指以人们碎片化时间作为消费场景的一系列产品和服务所形成的商业形态，其范围可以覆盖至睡眠舱、按摩椅 、移动KTV等）。因此，从产品形态来讲，盲盒属于典型的“惊喜经济”，但又具有一定的“潮玩经济”、“等待经济”跨界属性。<br>1.2盲盒发展历程<br>（1）盲盒的消费文化起源。从消费文化视角看，盲盒经济源于历史上的收藏文化，如早期的古董收藏到后来的集邮。<br>（2）盲盒模式发展溯源。从商业模式视角看，盲盒经济最早可以追溯到最古老的彩票，以及近十多年国内的赌石也可算是其模式上的早期代表，炒鞋、娃娃机则可以看成是最典型的盲盒经济雏形。<br>（3）盲盒经济的发展孕育过程。现代意义上的盲盒可追溯至明治末期的日本，以当时的百货公司新年期间销售的“福袋”为典型代表。进入80年代，日本模型爱好者文化催生了“扭蛋机”等商品形态。步入90年代，年轻人受欧美、日韩的消费品牌及流行文化下的影响，形成了一定的群体文化共性，这为精神属性物品提供了很好的市场空间，诞生了90年代末开始的一批早期“集卡”爱好者，如原创IP小浣熊、影视游戏文化作品纪念卡，甚至也衍生出了线下换卡活动。此后，随着4G技术下移动互联网的发展，跨越地域范围的泛文化社群开始形成，以80、90后为主要群体的精神属性商品形成了一波新消费浪潮。早期的IP产品多采用明码标价的高溢价售卖形式，以文化记忆和群体分享为主。而随着08年金融危机后，一方面中高端游戏产业整体进入品牌化、IP化状态，竞争加剧；另一方面以95后、00后为代表的年轻消费人群成长于物质丰富年代，消费力强，且普遍受ACG文化影响；互联网也进入深度社交化状态。这样，借助早期娃娃机等商业模式的成功经验，带有一定运气成分的早期盲盒应运而生。但很快，经过一系列的演化，“彩票”的低概率“猜赌机制”不断注入各类盲盒产品，让“盲盒”这一纯商业模式创新型产品迅速走红。<br>\2. 当前盲盒经济现状<br>2.1产品形态<br>盲盒经济是典型的非功能性消费产品，多以一些玩具、模型等装饰物品为主。盲盒成为很多消费者关注潮玩产业的一个关键品类。然而，近些年，中国市场上出现的盲盒产品发生了变种，变成了一个刺激连续购买的瘾性消费商业游戏。盲盒产业链的核心是围绕IP进行商业开发，通过与电影、动漫、游戏等热门 IP 联名，以及签约原创设计师等方式，不断推出“爆款” 产品。产品形态主要为各类IP衍生的玩偶、摆件、模型等硬件产品为主。<br>2.2产品特点<br>当前，主流盲盒多以小件IP产品为主。业内人士通常根据抽中概率不同，将盲盒产品分为小隐、大隐和超隐等类型。早期的盲盒基本都可以界定为小隐模式，但随着对利润的追求，大隐、超隐模式很快就流行起来。以某一线盲盒品牌为例，其就是相同包装里，装着不同样式玩偶的盒子，只有拆开包装，才知道具体款式。一个系列盲盒通常有 12 个常规造型，加上 1 个隐藏款造型，隐藏款抽中的概率约 为 1&#x2F;144。<br>2.3产业链结构<br>从商品分类来看，盲盒产品属于跨越玩具和工艺品的一个衍生分支产品，主要发源于玩具产业，可以简要概括为玩具产业的一种营销模式创新。一般而言，盲盒产业链依次包括IP设计、IP运营、实物制造、实物零售、售后服务（换娃、改娃）、闲置交易6个环节。其中，IP设计、运营环节是价值链顶端，控制着盲盒产品的游戏规则，也是整个市场的主导者；零售环节的商家承担了线上线下的零售环节的流通角色，多半采用授权经销或直营连锁经销形式，销售形式有实体门店、无人售货机等。头部公司通常运营着自己的原创IP以及采购一些知名爆款内容类IP进行开发经营，后者联名开发；售后服务则相对狭窄和社群化；闲置交易主要在通用型闲置交易平台进行。<br>2.4营销方式<br>由于盲盒的实体产品属性，盲盒营销方式覆盖了线上线下渠道。与此同时，还创新了多种营销方式，如通过开发线上抽盲盒小程序、知名博主录制开箱视频、参与线下玩具展览等方式， 吸引潮玩爱好者的关注。甚至衍生出了独特的“改娃”、“换娃”等互动教程、及交换、代购等活动。从销售渠道看，早期主要以线下（线下门店、无人零售、展览）为主，线上（在线商城）份额处于持续增长状态，截至2019年，某一线品牌的线上营收比例已达32%。<br>2.5主流用户群体<br>盲盒消费客群为泛娱乐消费群体，年龄集中在为15-40岁之间，主力是被人们定义为Z世代（ACG，即漫画、动画、游戏陪伴下成长起来的一代）的95后人群。盲盒消费群体的消费能力普遍较强，且消费特点表现为非功能化、消费的形式化、行为化。细分来看，盲盒消费者可分为两类：一类是“忠实粉丝”，他们受产品特殊机制影响，导致潜移默化地消费“成瘾”，导致在其中产生巨额消费。另一部分是投机心态的群体，由于产品“隐”的稀缺性导致一些特别款产品在二手市场也在水涨船高的现象（如某品牌隐藏款产品原价59元，在闲鱼却卖到2350元的高价），这吸引了一些人在流通环节“跟风投资”。网络销售数据佐证了盲盒消费群体的特点，在天猫发布的《95后玩家剁手力榜单》中，潮玩手办的烧钱指数位列第一，成为95后年轻人中热度最高也最烧钱的爱好。仅在天猫就有近20万消费者每年花费2万余元收集盲盒，其中购买力最强的消费者一年购买盲盒甚至耗资百万。<br>2.6行业规模与增长<br>据广发证券研究显示，2019年，中国泛娱乐市场规模高达9166亿元，其中实物领域高达325亿元，潮玩市场规模为207亿元。从增长数据来看，2015-2019年，中国潮流玩具市场年复合增长率为34.6%。这表明，宏观行业还存在增长空间，尤其线上增长空间有待进一步开发。另据该行业头部公司招股书信息显示，其在2018年以来的两年间，实现了超过10倍的营收增长，超过280倍的利润增长，用户复购率达58%。这在2019-2020年，众多一线IP运营商纷纷加码盲盒产品，资本进入该行业，行业竞争已初步显现激烈态势。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/05/%E5%B8%82%E5%9C%BA%E5%88%86%E6%9E%90-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nekkoya">
      <meta itemprop="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵咪妈咪哄">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/05/%E5%B8%82%E5%9C%BA%E5%88%86%E6%9E%90-1/" class="post-title-link" itemprop="url">市场分析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-05 14:00:42 / 修改时间：14:00:57" itemprop="dateCreated datePublished" datetime="2022-10-05T14:00:42+08:00">2022-10-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>4.1行业规模与竞争趋势<br>目前，盲盒经济的直接用户集中与Z世代消费群体，购买需求以满足好奇、陪伴、社交分享和赠礼为主。行业正处于高速增长的原始发展阶段。来自广发证券的数据显示，预计在2024年，潮玩市场规模将增长至763亿元，这意味着，潮玩市场的分支——盲盒产品所在领域依然有较大的市场规模和增长空间。2019年开始，紧随着行业代表性企业的成功，众多IP运营商加速入场，将在短期内加剧行业竞争，形成利润回归。由于盲盒产品的特殊性，市场前景尚存变数。<br>4.2产品趋势<br>盲盒产品将会逐渐回归相对理性的状态，长期看“大隐、超隐”的产品不可持续。首先是随着市场逐渐成熟，大隐、超隐模式产品可能会得到被动改善。其次是随着巨额资本的涌入，必然导致供给过剩，产品将变得多元化，开始出现横向扩展，预计将有更多产品加入到“盲盒”模式中来，盲盒将从渠道竞争转向垂直品类竞争、IP创新和运营能力的竞争。比如，有业内人士就认为，原创潮流 IP 生命力比较短，一般能维持 3~5 年，内容 IP的由于版权方对 IP热度的维持，生命线会更长；同时，也有人认为，盲盒本质上是一种游戏机制，企业需要有从IP转向品牌运营的能力， 才能把品牌价值做深， 否则会是昙花一现。再次是随着市场的成熟，消费者会越来越理性，加上其他明码标价的域外产品的文化属性逐渐增强，某种程度上会对小众的盲盒产品形成挤压。此外，品控也成为资深玩家关注的焦点，产品品质很容易影响用户口碑。一些玩家表示，由于价格不菲，一旦IP产品品控没有做好或设计更新难以满足需求，消费者会因失望造成逆反心理，引发“退坑”。<br>4.3社会的质疑与监管<br>随着市场上“大隐、超隐”类产品逐渐增多，后期极有可能面临市场监管问题。早在2019 年 12 月 11 日央视财经频道《经济半小时》栏目的《疯狂的“韭菜” 盒子》节目中，就有业内研究者指出“盲盒经济”在中国正变种成为“博彩经济”。一些人认为，盲盒并非消费驱动，而是由供给市场推动的消费模式创新；也非文化诉求，而是商家的投机诉求。也有人认为，“新媒体的羊群效应”助长了消费者的非理性消费，将其定位为“玩偶+彩票+社交门票”的一类商品。与此同时，产品质量问题也引发人们关注，2019年10月16日，北京的盲盒发烧友就因质量问题发生了投诉。因此，社会的质疑并非空穴来风，当前一些盲盒产品的概率机制带有部分“打法律擦边球”的属性。直接受众又有不少是未成年学生群体，其中的成瘾机制长期看会面临监管的压力；供给端部分产品潜藏的品质问题、市场操控行为可能面临潜在法律风险；消费端的跟风炒作则更存在一定的投资风险。<br>第一节行业概况<br>一、盲盒定义<br>“盲盒”，顾名思义，就是在非透明的包装中加入物品，借由消费者的“赌博”心理而实现商品的售出。盲盒起源于日本的“福袋”，最开始是超市为了处理滞销的货物而将其放入不透明的袋子中，通过塑造不确定感来吸引客户的购买。这时候的福袋中往往是袋内物品的实际价值高于福袋的定价。随着日本二次元的兴起，包含各种手办玩偶的“扭蛋机”也随之出现。到20世纪90年代，这一类“盲盒”概念开始以集卡的形式出现在我国，尤其在学生群体中引起一阵消费热潮。<br>盲盒里面的产品不确定性很大，且多以系列形式推出，成为新一代年轻人娱乐、沟通、交换和购买的载体，占据了年轻群体的“惊喜经济”、“孤独经济”、“社交经济”等等。受高毛利率、高增长、创新的销售模式以及新兴的千亿青年潮玩市场等因素的吸引，资本、上市公司纷纷抢滩盲盒市场，盲盒市场前景广阔。<br>二、盲盒行业发展历程<br>萌芽期（1920-2005）：20世纪初，日本百货公司用不公开内容的“福袋”作为年末处理商品尾货的方式，盲盒雏形初现；90年代，盲盒以集卡的形式引入国内，在学生群体里风靡一时；1999年KAWS推出首款玩偶，潮流玩具开始出现。<br>蓄力期（2006-2015）：2005年，Dreams公司退出SonnyAngel系列玩具；2010年以后，泡泡玛特、19八3、IP小站等一系列潮玩公司相继成立；2015年，泡泡玛特引入SonnyAngel系列玩具，为其成为潮玩行业巨头打下基础。<br>爆发期（2016-至今）：2016年，泡泡玛特先后获得Molly、Pucky等热门IP的合作，推出一系列盲盒；2017年，首届北京潮流玩具展举办，潮玩文化开始形成；2020年12月，泡泡玛特在港股上市，是第一家以盲盒起家的上市潮玩公司。<br>第二节盲盒行业市场分析<br>一、盲盒市场发展现状<br>2021年，我国国内生产总值（GDP）达114.37万亿元，人均国内生产总值80976元，人均GDP突破8万元人民币，按年平均汇率折算，突破了1.25万美元，已经超过世界人均GDP水平。在物质生活得到满足后，以“盲盒”为代表的潮流玩具和时尚消费产业，正成为年轻人寻找存在感、进行社交的重要方式。<br><img src="/%E5%B8%82%E5%9C%BA%E5%88%86%E6%9E%90.assets/v2-b71c1a69c6aeec31395da424865c5ac9_720w.webp" alt="img"><br>图表1：2016-2021年全国GDP总量和增速<br>在可支配收入增加、潮流文化产业迅速发展，尤其是越来越多优质潮流玩具IP于市场成功孵化的驱动下，国内潮玩市场的规模从2015年的63亿元，增加到了2020年的294.8亿元，复合年增长率高达36%。预计市场规模在2024年会达到763亿元，2030年将突破1100亿元，并预期将于2024年达到2494.8亿元。作为潮玩的重要细分领域，盲盒市场也将获得较快的发展。<br>中国潮流市场仍处于早期阶段，并在过去数年获得了较快的发展。数据显示，2019年中国潮玩市场规模为207亿元，同比2018年增长了47.9%，预计受疫情影响，2020年中国潮玩市场规模增速有所下降，市场规模增至262亿元，2018-2020年，中国盲盒行业市场规模保持持续增加的趋势，从2018年的51.3亿元增加到2020年的101亿元。2018-2020年，中国盲盒行业市场规模年均复合增速约为40.31%。受惊喜经济和社交情感需求等影响，中国潮玩受欢迎程度不断上升，预计中国潮玩市场规模将保持扩张态势，我国盲盒行业至少还会迎来5年的高速增长期。<br><img src="/%E5%B8%82%E5%9C%BA%E5%88%86%E6%9E%90.assets/v2-89feb602d354aff55d8aa29251d814fb_720w.webp" alt="img"><br>图表2：2012-2021年中国潮流玩具注册成立公司情况<br>二、盲盒主要消费者<br>潮流玩具标签受欢迎程度前三名依次为：设计师原创IP、影视作品IP和动漫形象IP，其中设计师原创IP得分最高，为80.76%，显示出极高市场竞争力。因此，潮流玩具企业需要重视艺术家的创作能力和经验，自主开发具有代表性标签玩具的衍生品。<br><img src="/%E5%B8%82%E5%9C%BA%E5%88%86%E6%9E%90.assets/v2-8eae8ede087fb5b58c3ce0320f674b0a_720w.webp" alt="img"><br>图表3：最受欢迎的潮玩盲盒IP产品<br>泡泡玛特官方提供的用户数据显示，盲盒主要消费者职业以白领为主，占比33.20%。其他主要消费者中，学生占25.2%，个体经营占8.7%，教职人员占12%，其他职业占20.9%。大部分年轻消费者具有良好的教育背景，追求消费品质。<br>从年龄来看，18岁至24岁的用户占32%，25岁至29岁的占26%，30岁至34岁的占20%，其他年龄段占22%。其中，1995年后出生的女性消费者是潮玩消费的主力军。<br>盲盒消费人群主要集中在一二线城市及各大发达省份。据艾媒咨询，盲盒的主要消费者是高购买力的年轻女性，因此北京、广东、浙江等发达地区占比较高。闲鱼2019年7月的统计盲盒前五大交易收发地为依次上海、北京、广州、天津、杭州，均为一线、新一线城市。高消费、高知群体构成盲盒潮玩主要消费力的一大原因是这类群体更多地接触多元文化、现代生活方式和海量信息，因此对自我表达、展现个性拥有更高需求，也更愿意进行情感满足式、奖励式的消费行为。<br>社交和自我愉悦是消费者购买潮玩盲盒的主要动机，而非交易投资。46.78%的消费者认为惊喜感是其购买潮玩盲盒的重要原因，38.95%的消费者认为购买潮玩盲盒是因为有其喜欢的标签，21.64%的消费者认为购买潮玩盲盒是因为和亲友一起选购有趣。至于送人和交易投资，多数消费者认为这并不是其购买潮玩盲盒的主要动机，仅有1.47%的消费者将二手交易视为购买动机。<br>数据显示，2021年购买潮玩盲盒总花费在1000元以下的消费者占31.11%，购买潮玩盲盒总花费在5000元以下消费者占64.03%。这与消费者的理性、注重实用特征密不可分。<br>第三节盲盒行业未来发展趋势<br>一、盲盒核心竞争力<br>1、强惊喜感<br>盲盒独特的购买方式，使得用户拆盲盒时会对未知的款式附赠强烈的惊喜感，甚至这种惊喜感的价值会高于商品本身。<br>2、强社交属性<br>当用户购买到重复的、不喜欢的盲盒款式，从而产生置换盲盒的需求，盲盒社交圈因此成型。娃友们会在线上讨论新出的盲盒款式，会举办线下聚会来展示稀有盲盒IP。<br>3、强溢价<br>物以稀为贵，出现概率低的“隐藏款”盲盒在二级市场供不应求，价格被炒上天。用户抽到“隐藏款”盲盒后，可以收获到比原价高出几倍的收益。炒盲盒成为新型的理财方式。<br>二、盲盒行业发展趋势<br>1、中国盲盒市场消费潜力持续提升<br>数据显示，从中国潮玩市场规模占全球市场规模的比例来看，占比从2017年的11.18%升至2020年的19.74%，预计到2023年有望达到23.03%。艾媒咨询分析师认为，国内围绕影视、动漫、游戏等IP周边授权商品的潮玩产业正处于高速发展之中，作为潮玩IP变现的重要载体，盲盒市场将借势发展，并有望逐渐发展成为全球盲盒消费的核心市场之一。<br><img src="/%E5%B8%82%E5%9C%BA%E5%88%86%E6%9E%90.assets/v2-cc362fbc3b6236112a84bf5ad31229e9_720w.webp" alt="img"><br>图表4：2017-2023年中国潮玩行业市场规模占全球市场规模的比例及预测<br>2、年轻群体带动盲盒发展<br>年轻群体对盲盒的兴趣，正转化为庞大的消费力。数据显示，95后最“烧钱”的爱好中，盲盒手办排名第一。95后成为盲盒的重要消费用户，占比近4成，其中8.6%的用户可以接受盲盒单价在1000元以上，接近20%的倾向于一次性购买全套盲盒，这都凸显了年轻群体旺盛的消费力。<br>个性化需求+消费心理变化，年轻群体成为IP衍生行业消费主力军。IP通常产自ACG（动画（Animation），漫画（Comic），游戏（Game）），相关衍生产品细分赛道众多，满足了年轻群体日益高涨的个性化需求。同时，年轻人的消费心理发生了潜移默化的改变，越发注重情怀、童年等精神需求，丰富的IP衍生品市场填补了年轻群体的需求空缺，如潮流玩具已成为年轻消费者心中时尚、艺术、文化的代表。年轻消费群体与IP衍生市场双向赋能，IP衍生市场满足年轻群体的消费需求，年轻群体的消费促进IP衍生市场的进一步完善。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/05/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nekkoya">
      <meta itemprop="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵咪妈咪哄">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/05/%E5%95%86%E4%B8%9A%E6%A8%A1%E5%BC%8F-1/" class="post-title-link" itemprop="url">商业模式</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-05 14:00:20 / 修改时间：14:00:29" itemprop="dateCreated datePublished" datetime="2022-10-05T14:00:20+08:00">2022-10-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>第一市场：购买盲盒定制化服务<br>第二市场：二手市场 -&gt; 或将占据盲盒市场，引领新的潮流<br>第三市场：小红书、抖音等分享拆盒经历、改造盲盒等，实现流量变现</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/05/A-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nekkoya">
      <meta itemprop="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵咪妈咪哄">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/05/A-1/" class="post-title-link" itemprop="url">A</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-05 13:59:56 / 修改时间：14:00:11" itemprop="dateCreated datePublished" datetime="2022-10-05T13:59:56+08:00">2022-10-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>充分性<br>上面我们介绍了如何用CLIP实现zero-shot分类，下面将简单介绍CLIP与其它方法的效果对比。首先是CLIP和17年的一篇工作<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1612.09161">Learning Visual N-Grams from Web Data</a>的在3个分类数据集上zero-shot效果对比，如下表所示，可以看到CLIP模型在效果上远远超过之前的模型，其中在ImageNet数据集可以达到76.2，这和全监督的ResNet50效果相当，不用任何训练数据就能达到这个效果是相当惊艳的。<br><img src="/A.assets/v2-d3c1c6676c28f75d26029c0cde43ed60_720w.webp" alt="img"><br>更进一步地，论文还对比了zero-shot CLIP和ResNet50 linear probing（ImageNet数据上预训练，在加上线性分类层进行finetune）在27个数据集上表现，如下图所示，其中在16个数据集上CLIP可以超过ResNet50。但是在一些特别的，复杂的或者抽象的数据集上CLIP表现较差，比如卫星图像分类，淋巴结转移检测，在合成场景中计数等，CLIP的效果不如全监督的ResNet50，这说明CLIP并不是万能的，还是有改进的空间。<br><img src="/A.assets/v2-dc8ba977117fdec5fd2f79961d7110d4_720w.webp" alt="img"><br>除了zero-shot对比，论文还对比few-shot性能，即只用少量的样本来微调模型，这里对比了3个模型：在ImageNet21K上训练的BiT-M ResNet-152x2，基于SimCLRv2训练的ResNet50，以及有监督训练的ResNet50。可以看到CLIP的zero-shot和最好的模型（BiT-M）在16-shot下的性能相当，而CLIP在16-shot下效果有进一步的提升。另外一个比较有意思的结果是：虽然CLIP在few-shot实验中随着样本量增加性能有提升，但是1-shot和2-shot性能比zero-shot还差，这个作者认为主要是CLIP的训练和常规的有监督训练存在一定的差异造成的。<br><img src="/A.assets/v2-4d122ef5767f0453121698d4bad08036_720w.webp" alt="img"><br>除此之外，论文还进行了表征学习（representation Learning）实验，即自监督学习中常用的linear probe：用训练好的模型先提取特征，然后用一个线性分类器来有监督训练。下图为不同模型在27个数据集上的average linear probe score对比，可以看到CLIP模型在性能上超过其它模型，而且计算更高效：<br><img src="/A.assets/v2-42d05e933b46aebf16ba11e2cfce6154_720w.webp" alt="img"><br>另外，论文还发现CLIP在自然分布漂移上表现更鲁棒，比如CLIP和基于ImageNet上有监督训练的ResNet101在ImageNet验证集都能达到76.2%，但是在ImageNetV2数据集上，CLIP要超过ResNet101。在另外的4个分布漂移的数据集上，ResNet101性能下降得比较厉害，但是CLIP能依然保持较大的准确度，比如在ImageNet-A数据集上，ResNet101性能只有2.7%，而CLIP能达到77.1%。<br><img src="/A.assets/v2-7798ab2d513a0ab79117adcf74d8ec05_720w.webp" alt="img"><br>CLIP能实现这么好的zero-shot性能，大家很可能质疑CLIP的训练数据集可能包含一些测试数据集中的样例，即所谓的数据泄漏。关于这点，论文也采用一个重复检测器对评测的数据集重合做了检查，发现重合率的中位数为2.2%，而平均值在3.2%，去重前后大部分数据集的性能没有太大的变化，如下所示：<br><img src="/A.assets/v2-dcbbf5bc5e3f0e3ff60155547fd134b8_720w.webp" alt="img"><br>论文的最后也对CLIP的局限性做了讨论，这里简单总结其中比较重要的几点：<br>●CLIP的zero-shot性能虽然和有监督的ResNet50相当，但是还不是SOTA，作者估计要达到SOTA的效果，CLIP还需要增加1000x的计算量，这是难以想象的；<br>●CLIP的zero-shot在某些数据集上表现较差，如细粒度分类，抽象任务等；<br>●CLIP在自然分布漂移上表现鲁棒，但是依然存在域外泛化问题，即如果测试数据集的分布和训练集相差较大，CLIP会表现较差；<br>●CLIP并没有解决深度学习的数据效率低下难题，训练CLIP需要大量的数据；<br>为什么是CLIP（必要性）<br>前面介绍了CLIP的原理和应用，这里我们再回过头来看另外一个问题：为什么是CLIP，即CLIP这篇工作的motivation。 在计算机视觉领域，最常采用的迁移学习方式就是先在一个较大规模的数据集如ImageNet上预训练，然后在具体的下游任务上再进行微调。这里的预训练是基于有监督训练的，需要大量的数据标注，因此成本较高。近年来，出现了一些基于自监督的方法，这包括基于对比学习的方法如MoCo和SimCLR，和基于图像掩码的方法如MAE和BeiT，自监督方法的好处是不再需要标注。但是无论是有监督还是自监督方法，它们在迁移到下游任务时，还是需要进行有监督微调，而无法实现zero-shot。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，所以在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务往往是辅助来进行表征学习，在迁移到其它数据集时也需要加上新的分类器来进行有监督训练。但是NLP领域，基于自回归或者语言掩码的预训练方法已经取得相对成熟，而且预训练模型很容易直接zero-shot迁移到下游任务，比如OpenAI的GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另外一个原因就是NLP模型可以采用从互联网上收集的大量文本。那么问题来了：能不能基于互联网上的大量文本来预训练视觉模型？<br>那么其实之前已经有一些工作研究用文本来作为监督信号来训练视觉模型，比如16年的工作<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1511.02251">Learning Visual Features from Large Weakly Supervised Data</a>将这转化成一个多标签分类任务来预测图像对应的文本的bag of words；17年的工作<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1612.09161">Learning Visual N-Grams from Web Data</a>进一步扩展了这个方法来预测n-grams。最近的一些工作采用新的模型架构和预训练方法来从文本学习视觉特征，比如<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2006.06666">VirTex</a>基于transformer的语言模型，<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2008.01392">ICMLM</a>基于语言掩码的方法，<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.00747">ConVIRT</a>基于对比学习的方法。整体来看，这方面的工作不是太多，这主要是因为这些方法难以实现较高的性能，比如17年的那篇工作只在ImageNet上实现了11.5%的zero-shot性能，这远远低于ImageNet上的SOTA。另外，还有另外的是一个方向，就是基于文本弱监督来提升性能，比如谷歌的<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1912.11370">BiT</a>和<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.11929">ViT</a>基于JFT-300M数据集来预训练模型在ImageNet上取得SOTA，JFT-300M数据集是谷歌从互联网上收集的，通过一些自动化的手段来将web text来转化成18291个类别，但是存在一定的噪音。虽然谷歌基于JFT-300M数据集取得了较好的结果，但是这些模型依然采用固定类别的softmax分类器进行预训练，这大大限制了它的迁移能力和扩展性。<br>作者认为谷歌的弱监督方法和之前的方法的一个重要的区别在于规模，或者说算力和数据的规模不同。JFT-300M数据量达到了上亿级别，而且谷歌用了强大的算力来进行预训练。而VirTex，ICMLM和ConVIRT只在10万级别的数据上训练了几天。为了弥补数据上的差异，OpenAI从网上收集了4亿的数据来实验。但是新的问题来了：采用什么样的方法来训练。OpenAI首先尝试了VirTex模型，即联合训练一个CNN和文本transformer来预测图像的文本（image caption），但是发现这种方法的训练效率（用ImageNet数据集上的zero-shot性能来评估）还不如直接预测bag of words，如下图所示，两者的训练效率能相差3倍。如果进一步采用<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.00747">ConVIRT</a>，即基于对比学习的方法，训练效率可以进一步提升4倍。之所出现这个差异，这不难理解，训练数据所包含的文本-图像对是从互联网收集来的，它们存在一定的噪音，就是说文本和图像可能并不完全匹配，这个时候适当的降低训练目标，反而能取得更好的收敛。而从任务难度来看：Transformer Language Model &gt; Bag of Words Prediction &gt; Bag of Words Contrastive (CLIP)。由于训练数据量和模型计算量较大，训练效率成为一个至关重要的因素。这就是作者最终选择对比学习的方法来训练的原因。<br><img src="/A.assets/v2-0a2056e539c727c3be2ae1c93829d118_720w.webp" alt="img"><br>从本质上来讲，CLIP其实并没有太大的创新，它只是将<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.00747">ConVIRT</a>方法进行简化，并采用更大规模的文本-图像对数据集来训练。<br>在论文的最后，作者也谈到了由于训练效率的制约，他们采用了对比学习的方法，但是他们依然想做的是直接用图像生成文本，这个如果能成功，那么就和DALL-E这个工作形成闭环了：文本 -&gt; 图像 -&gt; 文本。而且基于生成式训练出来的模型，同样可以实现zero-shot分类，我们可以通过预测句子中的单词（标签）来实现：A photo of [?]。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/05/Clip/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nekkoya">
      <meta itemprop="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵咪妈咪哄">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/05/Clip/" class="post-title-link" itemprop="url">Clip</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-05 13:59:15 / 修改时间：13:59:38" itemprop="dateCreated datePublished" datetime="2022-10-05T13:59:15+08:00">2022-10-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="一、运作流程"><a href="#一、运作流程" class="headerlink" title="一、运作流程"></a>一、运作流程</h3><p>用户可以在线上或线下机器输入图片以及文字描述，结合我们提供的裸娃图，通过本部本地搭建的disco diffusion处理，显示5张不同批次的图片及其实现的过程，通过感知哈希算法判断最终图与原裸娃图的相似性，系统判断相似度是否达到50%（后期可以实验调整），留下达标图片，再交由设计师处理、选择，以及建模等进一步精细化设计，做出3D设计稿，部件分拆（一般不需要分拆），3D打印手板件，手板打磨上底色，样板上色，组装，邮寄。</p>
<h3 id="二、原理介绍"><a href="#二、原理介绍" class="headerlink" title="二、原理介绍"></a>二、原理介绍</h3><h5 id="（Ⅰ）汉明距离（Hamming-distance）"><a href="#（Ⅰ）汉明距离（Hamming-distance）" class="headerlink" title="（Ⅰ）汉明距离（Hamming distance）"></a>（Ⅰ）汉明距离（Hamming distance）</h5><p>在信息论中，两个等长字符串之间的汉明距离是两个字符串对应位置的不同字符的个数。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。我们也可以将汉明距离理解为两个等长字符串之间将其中一个变为另外一个所需要作的最小替换次数。</p>
<p>布雷汉明距离的Python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">HammingDistance</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(x_ch != y_ch <span class="keyword">for</span> x_ch, y_ch <span class="keyword">in</span> <span class="built_in">zip</span>(x, y))</span><br></pre></td></tr></table></figure>

<h5 id="（Ⅱ）感知哈希算法（perceptual-hash-algorithm）"><a href="#（Ⅱ）感知哈希算法（perceptual-hash-algorithm）" class="headerlink" title="（Ⅱ）感知哈希算法（perceptual hash algorithm）"></a>（Ⅱ）感知哈希算法（perceptual hash algorithm）</h5><p>感知哈希算法(perceptual hash algorithm)，它的作用是对每张图像生成一个“指纹”(fingerprint)字符串，然后比较不同图像的指纹。结果越接近，就说明图像越相似。</p>
<p>实现步骤：</p>
<ol>
<li>缩小尺寸：将图像缩小到8*8的尺寸，总共64个像素。这一步的作用是去除图像的细节，只保留结构&#x2F;明暗等基本信息，摒弃不同尺寸&#x2F;比例带来的图像差异；</li>
<li>简化色彩：将缩小后的图像，转为64级灰度，即所有像素点总共只有64种颜色；</li>
<li>计算平均值：计算所有64个像素的灰度平均值；</li>
<li>比较像素的灰度：将每个像素的灰度，与平均值进行比较，大于或等于平均值记为1，小于平均值记为0；</li>
<li>计算哈希值：将上一步的比较结果，组合在一起，就构成了一个64位的整数，这就是这张图像的指纹。组合的次序并不重要，只要保证所有图像都采用同样次序就行了；</li>
</ol>
<p>得到指纹以后，就可以对比不同的图像，看看64位中有多少位是不一样的。在理论上，这等同于”汉明距离”(Hamming distance,)。如果不相同的数据位数不超过5，就说明两张图像很相似；如果大于10，就说明这是两张不同的图像。</p>
<h5 id="（Ⅲ）Disco-Diffusion"><a href="#（Ⅲ）Disco-Diffusion" class="headerlink" title="（Ⅲ）Disco Diffusion"></a>（Ⅲ）Disco Diffusion</h5><p>disco diffusion是发布于 Google Colab 平台的一款利用人工智能深度学习进行数字艺术创作的工具，它是基于 MIT 许可协议的开源工具，可以在 Google Drive 直接运行，也可以部署到本地运行，目前最新的版本是 Disco Diffusion v5.2。</p>
<p>原理是「CLIP-Guided Diffusion」，通过文字输入让AI产生相应的图片输出。</p>
<h5 id="（Ⅳ）CLIP"><a href="#（Ⅳ）CLIP" class="headerlink" title="（Ⅳ）CLIP"></a>（Ⅳ）CLIP</h5><p>其中CLIP(Contrastive Language-lmage Pre-training)是用于标记图像的工具。结合使用时，CLIP使用其图像识别技术迭代地引导Diffusion去噪过程朝向与文本提示紧密匹配的图像。Diffusion是一个迭代的过程。每次迭代时，CLIP都会根据文字提示，评估现有的图像，并为Diffusion提供「方向」。Diffusion将对现有图像进行「去噪」。所以你运行DD时会看到，图像最初只是一团模糊的混乱，但随着DD在迭代时间步长中的推进，图像的粗略和精细细节将出现。</p>
<p>CLIP的英文全称是<strong>Contrastive Language-Image Pre-training</strong>，即<strong>一种基于对比文本-图像对的预训练方法或者模型</strong>。CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述。如下图所示，CLIP包括两个模型：<strong>Text Encoder</strong>和<strong>Image Encoder</strong>，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer。</p>
<p>这里对提取的文本特征和图像特征进行对比学习。对于一个包含N个文本-图像对的训练batch，将N个文本特征和N个图像特征两两组合，CLIP模型会预测出N2个可能的文本-图像对的相似度，这里的相似度直接<strong>计算文本特征和图像特征的余弦相似性（cosine similarity）</strong>，即上图所示的矩阵。这里共有N个正样本，即真正属于一对的文本和图像（矩阵中的对角线元素），而剩余的N2−N个文本-图像对为负样本，那么CLIP的训练目标就是最大N个正样本的相似度，同时最小化N2−N个负样本的相似度，对应的伪代码实现如下所示：</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># image_encoder - ResNet or Vision Transformer</span></span><br><span class="line"><span class="comment"># text_encoder - CBOW or Text Transformer</span></span><br><span class="line"><span class="comment"># I[n, h, w, c] - minibatch of aligned images</span></span><br><span class="line"><span class="comment"># T[n, l] - minibatch of aligned texts</span></span><br><span class="line"><span class="comment"># W_i[d_i, d_e] - learned proj of image to embed</span></span><br><span class="line"><span class="comment"># W_t[d_t, d_e] - learned proj of text to embed</span></span><br><span class="line"><span class="comment"># t - learned temperature parameter</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别提取图像特征和文本特征</span></span><br><span class="line"><span class="attr">I_f</span> = image_encoder(I) <span class="comment">#[n, d_i]</span></span><br><span class="line"><span class="attr">T_f</span> = text_encoder(T) <span class="comment">#[n, d_t]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化</span></span><br><span class="line"><span class="attr">I_e</span> = l2_normalize(np.dot(I_f, W_i), axis=<span class="number">1</span>)</span><br><span class="line"><span class="attr">T_e</span> = l2_normalize(np.dot(T_f, W_t), axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算缩放的余弦相似度：[n, n]</span></span><br><span class="line"><span class="attr">logits</span> = np.dot(I_e, T_e.T) * np.exp(t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对称的对比学习损失：等价于N个类别的cross_entropy_loss</span></span><br><span class="line"><span class="attr">labels</span> = np.arange(n) <span class="comment"># 对角线元素的labels</span></span><br><span class="line"><span class="attr">loss_i</span> = cross_entropy_loss(logits, labels, axis=<span class="number">0</span>)</span><br><span class="line"><span class="attr">loss_t</span> = cross_entropy_loss(logits, labels, axis=<span class="number">1</span>)</span><br><span class="line"><span class="attr">loss</span> = (loss_i + loss_t)/<span class="number">2</span></span><br></pre></td></tr></table></figure>

<p>为了训练CLIP，OpenAI从互联网收集了共<strong>4个亿的文本-图像对</strong>，论文称之为<strong>WebImageText</strong>，如果按照文本的单词量，它和训练GPT-2的WebText规模类似，如果从数量上对比的话，它还比谷歌的JFT-300M数据集多一个亿，所以说这是一个很大规模的数据集。CLIP虽然是多模态模型，但它主要是用来<strong>训练可迁移的视觉模型</strong>。论文中Text Encoder固定选择一个包含63M参数的text transformer模型，而Image Encoder采用了两种的不同的架构，一是常用的CNN架构ResNet，二是基于transformer的ViT，其中ResNet包含5个不同大小的模型：<strong>ResNet50</strong>，<strong>ResNet101</strong>，<strong>RN50x4</strong>，<strong>RN50x16</strong>和<strong>RNx64</strong>（后面三个模型是按照EfficientNet缩放规则对ResNet分别增大4x，16x和64x得到），而ViT选择3个不同大小的模型：<strong>ViT-B&#x2F;32</strong>，<strong>ViT-B&#x2F;16</strong>和<strong>ViT-L&#x2F;14</strong>。所有的模型都训练32个epochs，采用AdamW优化器，而且训练过程采用了一个<strong>较大的batch size：32768</strong>。由于数据量较大，最大的ResNet模型RN50x64需要在592个V100卡上训练18天，而最大ViT模型ViT-L&#x2F;14需要在256张V100卡上训练12天，可见要训练CLIP需要耗费多大的资源。对于ViT-L&#x2F;14，还在336的分辨率下额外finetune了一个epoch来增强性能，论文发现这个模型效果最好，记为<strong>ViT-L&#x2F;14</strong><a href=""><strong>@336</strong> </a> ，论文中进行对比实验的CLIP模型也采用这个。</p>
<p><strong>如何用CLIP实现zero-shot分类</strong></p>
<p>上面我们介绍了CLIP的原理，可以看到训练后的CLIP其实是两个模型，除了视觉模型外还有一个文本模型，那么如何对预训练好的视觉模型进行迁移呢？<strong>与CV中常用的先预训练然后微调不同，CLIP可以直接实现zero-shot的图像分类，即不需要任何训练数据，就能在某个具体下游任务上实现分类，</strong>这也是CLIP亮点和强大之处。用CLIP实现zero-shot分类很简单，只需要简单的两步：</p>
<ol>
<li>根据任务的分类标签构建每个类别的描述文本：A photo of {label}，然后将这些文本送入Text Encoder得到对应的文本特征，如果类别数目为N，那么将得到N个文本特征；</li>
<li>将要预测的图像送入Image Encoder得到图像特征，然后与N个文本特征计算缩放的余弦相似度（和训练过程一致），然后选择相似度最大的文本对应的类别作为图像分类预测结果，进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率。</li>
</ol>
<p>可以看到，我们是利用CLIP的多模态特性为具体的任务<strong>构建了动态的分类器</strong>，<strong>其中Text Encoder提取的文本特征可以看成分类器的weights，而Image Encoder提取的图像特征是分类器的输入</strong>。这里我们给出了一个基于CLIP的一个实例（参考官方<a href="https://link.zhihu.com/?target=https://github.com/openai/CLIP/blob/main/notebooks/Interacting_with_CLIP.ipynb">notebook</a>），这里任务共有6个类别：”dog”, “cat”, “bird”, “person”, “mushroom”, “cup”，首先我们创建文本描述，然后提取文本特征：</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先生成每个类别的文本描述</span></span><br><span class="line">labels = [<span class="string">&quot;dog&quot;</span>, <span class="string">&quot;cat&quot;</span>, <span class="string">&quot;bird&quot;</span>, <span class="string">&quot;person&quot;</span>, <span class="string">&quot;mushroom&quot;</span>, <span class="string">&quot;cup&quot;</span>]</span><br><span class="line">text_descriptions = [f<span class="string">&quot;A photo of a &#123;label&#125;&quot;</span> for label in labels]</span><br><span class="line">text_tokens = clip.tokenize<span class="params">(text_descriptions)</span><span class="string">.cuda</span><span class="params">()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取文本特征</span></span><br><span class="line">with torch.no_grad<span class="params">()</span>:</span><br><span class="line">    text_features = model.encode_text<span class="params">(text_tokens)</span><span class="string">.float</span><span class="params">()</span></span><br><span class="line">    text_features <span class="string">/=</span> text_features.norm<span class="params">(<span class="attr">dim</span>=-1, <span class="attr">keepdim</span>=True)</span></span><br></pre></td></tr></table></figure>

<p>然后我们读取要预测的图像，输入Image Encoder提取图像特征，并计算与文本特征的余弦相似度：</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取图像</span></span><br><span class="line">original_images = []</span><br><span class="line">images = []</span><br><span class="line">texts = []</span><br><span class="line"></span><br><span class="line">for label in labels:</span><br><span class="line">    image_file = os.path.join<span class="params">(&quot;images&quot;, label+&quot;.jpg&quot;)</span></span><br><span class="line">    name = os.path.basename<span class="params">(image_file)</span><span class="string">.split</span><span class="params">(&#x27;.&#x27;)</span>[0]</span><br><span class="line"></span><br><span class="line">    image = Image.open<span class="params">(image_file)</span><span class="string">.convert</span><span class="params">(&quot;RGB&quot;)</span></span><br><span class="line">    original_images.append<span class="params">(image)</span></span><br><span class="line">    images.append<span class="params">(preprocess(image)</span>)</span><br><span class="line">    texts.append<span class="params">(name)</span></span><br><span class="line"></span><br><span class="line">image_input = torch.tensor<span class="params">(np.stack(images)</span>)<span class="string">.cuda</span><span class="params">()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 提取图像特征  </span></span><br><span class="line">with torch.no_grad<span class="params">()</span>:</span><br><span class="line">    image_features = model.encode_image<span class="params">(image_input)</span><span class="string">.float</span><span class="params">()</span></span><br><span class="line">    image_features <span class="string">/=</span> image_features.norm<span class="params">(<span class="attr">dim</span>=-1, <span class="attr">keepdim</span>=True)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算余弦相似度（未缩放）</span></span><br><span class="line">similarity = text_features.cpu<span class="params">()</span><span class="string">.numpy</span><span class="params">()</span> @ image_features.cpu<span class="params">()</span><span class="string">.numpy</span><span class="params">()</span><span class="string">.T</span></span><br></pre></td></tr></table></figure>

<p>相似度如下所示，可以看到对于要预测的6个图像，按照最大相似度，其均能匹配到正确的文本标签：</p>
<p>进一步地，我们也可以对得到的余弦相似度计算softmax，得到每个预测类别的概率值，注意这里要对相似度进行缩放：</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">logit_scale = np.<span class="built_in">exp</span>(model.logit_scale.<span class="keyword">data</span>.item())</span><br><span class="line">text_probs = (logit_scale * image_features @ text_features.T).softmax(<span class="built_in">dim</span>=-<span class="number">1</span>)</span><br><span class="line">top_probs, top_labels = text_probs.cpu().topk(<span class="number">5</span>, <span class="built_in">dim</span>=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>得到的预测概率如下所示，可以看到6个图像，CLIP模型均能够以绝对的置信度给出正确的分类结果：</p>
<p>使用CLIP进行zero-shot分类，另外一个比较重要的地方是<strong>文本描述的生成</strong>，上面的例子我们采用A photo of {label}，但其实也有其它选择，比如我们直接用类别标签，这其实属于最近NLP领域比较火的一个研究：prompt learning或者prompt engineering，这和之前的预训练+微调属于不同的范式。如果我们直接采用类别标签作为文本描述，那么很多文本就是一个单词，缺少具体的上下文，而且也和CLIP的训练数据不太一致，效果上会不如采用A photo of {label}（ImageNet数据集上可以提升1.3%）。论文也实验了采用80个不同的prompt来进行集成，发现在ImageNet数据集上能带来3.5%的提升。下图对比了基于ResNet的CLIP模型直接采用类别名与进行prompt engineering和ensembling的效果对比：</p>
<h5 id="Ⅴ）3D打印"><a href="#Ⅴ）3D打印" class="headerlink" title="(Ⅴ）3D打印"></a>(Ⅴ）3D打印</h5><p>3D打印技术的工作原理是利用计算机建模软件(如3Dmax等。)在计算机上创建一个虚拟的3D模型，将模型文件转换成与3D打印机匹配的格式，然后3D打印机根据切片程序将横截面堆叠成3D模型来完成打印。</p>
<p>3D打印技术避免了传统制造业的切割工艺，不需要模具制造。加工速度快，生产周期相对较短。更重要的是，3D打印在制造体积小、结构复杂的物体方面有很大的优势。通过集成印刷技术，不需要二次加工，通过计算机在线操作实现批量生产和远程控制。</p>
<p>3D打印技术最具代表性的是熔积成型技术。其工作原理是通过送丝装置将热熔长丝材料送入喷嘴。在计算机软件的控制下，加热喷嘴挤出软化的材料，并开始沿着物体的轮廓移动，直到半流动材料的填充和固化完成，从而形成3D打印产品。例如，对于塑料制品的三维打印，通过加热三维打印机的喷嘴使塑料熔化，挤出后塑料迅速冷却，并与周围材料粘合和覆盖。目前，FDM可以印刷金属、石蜡、ABS、PLA、人造橡胶等,同时生产3D模型、机械零件、日用品等。广泛应用于建筑、汽车、航空航天和医疗领域。与传统机械加工生产相比，FDM技术具有成本低、材料广泛、原材料利用率高、污染少等优点。</p>
<h5 id="（Ⅵ）prompt-learning-x2F-prompt-engineering"><a href="#（Ⅵ）prompt-learning-x2F-prompt-engineering" class="headerlink" title="（Ⅵ）prompt learning&#x2F;prompt engineering"></a>（Ⅵ）prompt learning&#x2F;prompt engineering</h5><p>与传统的监督学习(训练模型接受输入x并预测输出y为P(y|x))不同，基于提示的学习基于直接模拟文本概率的语言模型。为了使用这些模型执行预测任务，使用模板将原始输入x修改为文本字符串提示符x’，其中有一些未填充的槽，然后使用语言模型概率地填充未填充的信息，以获得最终字符串x，从中可以导出最终输出y。该框架功能强大且具有吸引力，原因有很多:它允许语言模型在大量原始文本上进行预训练，通过定义一个新的提示函数，模型能够执行少次甚至零次学习，适应有很少或没有标记数据的新场景。</p>
<h5 id="（Ⅶ）Diffusion-Model"><a href="#（Ⅶ）Diffusion-Model" class="headerlink" title="（Ⅶ）Diffusion Model"></a>（Ⅶ）Diffusion Model</h5><p>各类生成模型对比图:</p>
<p><img src="/Clip%E6%A8%A1%E5%9E%8B.assets/v2-42181e6098a90635a05cfeb1c1091afe_720w.webp" alt="img"></p>
<p>diffusion model和其他模型最大的区别是它的latent code(z)和原图是同尺寸大小的（最近也有基于压缩的latent diffusion model）。diffusion model即存在一系列高斯噪声（ T 轮），将输入图片 x0 变为纯高斯噪声 xT 。而我们的模型则负责将 xT 复原回图片 x0 。要强调的是，这里噪声 xT 与图片x0是<strong>同维度</strong>的。</p>
<p>所谓前向过程，即往图片上加噪声的过程。虽然这个步骤无法做到图片生成，但是这是理解diffusion model以及<strong>构建训练样本GT</strong>至关重要的一步。</p>
<p>给定真实图片 x0∼q(x) ,diffusion前向过程通过 T 次累计对其添加高斯噪声，得到 x1,x2,…,xT ，如下图的q过程。这里需要给定一系列的高斯分布方差的超参数 {βt∈(0,1)}t&#x3D;1T .前向过程由于每个时刻 t 只与 t−1 时刻有关，所以也可以看做马尔科夫过程：</p>
<p>(1)q(xt|xt−1)&#x3D;N(xt;1−βtxt−1,βtI),q(x1:T|x0)&#x3D;∏t&#x3D;1Tq(xt|xt−1).</p>
<p>这个过程中，随着 t 的增大， xt 越来越接近纯噪声。当 T→∞ ， xT 是完全的高斯噪声（下面会证明，且与均值系数1−βt 的选择有关）。且实际中 βt 随着t增大是递增的，即 β1&lt;β2&lt;…&lt;βT 。</p>
<p>如果说前向过程(forward)是加噪的过程，那么逆向过程(reverse)就是diffusion的去噪推断过程。如果我们能够逐步得到逆转后的分布 q(xt−1|xt) ，就可以从完全的标准高斯分布 xT∼N(0,I) 还原出原图分布 x0 .在文献[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/525106459#ref_7">7]</a>中证明了如果 q(xt|xt−1) 满足高斯分布且 βt 足够小，q(xt−1|xt)仍然是一个高斯分布。然而我们无法简单推断q(xt−1|xt)，因此我们使用深度学习模型（参数为 θ ，目前主流是U-Net+attention的结构）去预测这样的一个逆向的分布 pθ（类似VAE） ：</p>
<p>(5-1)pθ(X0:T)&#x3D;p(xT)∏t&#x3D;1Tpθ(xt−1|xt); (5-2)pθ(xt−1|xt)&#x3D;N(xt−1;μθ(xt,t),Σθ(xt,t)).</p>
<p>虽然我们无法得到逆转后的分布 q(xt−1|xt)，但是如果知道 x0 ，是可以通过贝叶斯公式得到 q(xt−1|xt,x0) 为：</p>
<p>(6)q(xt−1|xt,x0)&#x3D;N(xt−1;μ(xt,x0),βtI)</p>
<p>过程如下：</p>
<h3 id="三、一些限定条件"><a href="#三、一些限定条件" class="headerlink" title="三、一些限定条件"></a>三、一些限定条件</h3><ol>
<li>每种娃限量，但有不同尺寸，手办或者bjd（流程中写的是手办制作流程，bjd类似）</li>
<li>同样的描述和图片可以得出相似的</li>
<li>用户可以选择一并收到3d模型（加钱 元宇宙）</li>
<li>两千多位艺术家风格可选择：<a target="_blank" rel="noopener" href="https://docs.google.com/spreadsheets/d/14xTqtuV3BuKDNhLotB_d1aFlBGnDJOY0BRXJ8-86GpA/edit#gid=0">https://docs.google.com/spreadsheets/d/14xTqtuV3BuKDNhLotB_d1aFlBGnDJOY0BRXJ8-86GpA/edit#gid=0</a></li>
</ol>
<h3 id="四、实现细节"><a href="#四、实现细节" class="headerlink" title="四、实现细节"></a>四、实现细节</h3><ol>
<li>添加删除配色colour scheme 让AI自由发挥的选项</li>
<li>长宽给定 （后期可以实验调整）</li>
<li>步数可以设为250， 保存间隔为50 （后期可以实验调整）</li>
<li>显示间隔为10，批次为5（后期可以实验调整）<br><img src="/Clip%E6%A8%A1%E5%9E%8B.assets/354824bcc1cc493188b2aacf8a179260.png" alt="img"></li>
</ol>
<h3 id="五、给用户的注"><a href="#五、给用户的注" class="headerlink" title="五、给用户的注"></a>五、给用户的注</h3><ol>
<li>权重：在描述词后加上不同的数字调整权重，如“雷云5：，火焰：5”，画面中它们呈现的比例就会有所对应；加上“景深：-2”关键词，会减弱画面的景深效果；输入“4K”则会在分辨率不变的情况下提升锐化值，模拟出更清晰的画面。</li>
<li>用关键词而非句子描述加入时间点 季节 气候 等</li>
<li>尽可能地输入形容词前缀</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/10/05/%E6%90%AD%E5%BB%BAblog/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="nekkoya">
      <meta itemprop="description" content="此博客正式搭建于2022年10月2日，欢迎光临~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="喵咪妈咪哄">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/05/%E6%90%AD%E5%BB%BAblog/" class="post-title-link" itemprop="url">搭建blog</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-10-05 13:58:09 / 修改时间：13:59:02" itemprop="dateCreated datePublished" datetime="2022-10-05T13:58:09+08:00">2022-10-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2.assets/1664603495481.png" alt="1664603495481"></p>
<h3 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h3><p>一、建立自己的本地网站</p>
<ol>
<li><p>安装git<a target="_blank" rel="noopener" href="https://gitforwindows.org/">https://gitforwindows.org/</a> </p>
<p>Git是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。<br>安装步骤：双击下载好的exe文件，一路next就好了。<br>安装好后，打开git bash，输入git version感受一下安装成功后输入命令的感觉。</p>
</li>
<li><p>安装node.js<a target="_blank" rel="noopener" href="https://nodejs.org/en/">https://nodejs.org/en/</a> </p>
<p>Hexo是基于nodeJS环境的静态博客，里面的npm工具很有用，所以还是把这玩意儿装了。 </p>
<p>安装步骤：也是一路next就好了。 </p>
</li>
<li><p>安装hexo</p>
<p>在E盘blog文件夹下Git Bash输入命令<code>npm i -g hexo</code></p>
</li>
<li><p>初始化网站所需文件</p>
<p><code>hexo init</code></p>
<p><img src="/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2.assets/1664614750014.png" alt="1664614750014"></p>
</li>
<li><p>运行自己的本地网站</p>
<p><code>hexo g</code></p>
<p><code>hexo s</code></p>
<p><img src="/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2.assets/1664615227624.png" alt="1664615227624">复制粘贴，就可以在本地运行自己的网站啦！</p>
</li>
</ol>
<p>二、将Github作为自己网站的服务器（服务器托管）</p>
<ol>
<li><p>在Github上创建一个仓库repository，格式为：yourname.github.io，其中yourname是自己的github名称</p>
</li>
<li><p>进入创建的仓库，点击setting，点击pages，可以发现一个专属于自己网址</p>
<p><img src="/%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2.assets/1664764621727.png" alt="1664764621727"></p>
</li>
<li><p>添加ssh密钥</p>
<p>给予你当前电脑访问你Github仓库的权限，直接使用用户名和密码不太安全，所以我们使用sshkey来解决本地和服务器的连接问题</p>
<p>在gitbash中输入</p>
<p>&#96;git config –global user.name”yourname”</p>
<p> <code>git config --global user.email&quot;youremail&quot; </code></p>
<p>继续输入：<code>ssh-keygen -t rsa -C&quot;1582485954@qq.com&quot; </code>，一路回车，生成ssh</p>
<p>在.ssh文件中找到id_rsa.pub的文件，复制粘贴，在Github上添加Key</p>
</li>
</ol>
<p><strong>至此，全部环境就搭建好啦，接下里开始搭建博客</strong></p>
<h3 id="搭建博客"><a href="#搭建博客" class="headerlink" title="搭建博客"></a>搭建博客</h3><ol>
<li><p>回到git bash，输入<code>npm install hexo-deployer-git --save</code>，这样才能将你写好的文章部署到github服务器上让别人浏览到</p>
</li>
<li><p>安装成功后，执行以下命令</p>
<p><code>hexo clean</code></p>
<p><code>hexo g</code></p>
<p><code>hexo d</code></p>
</li>
</ol>
<p><strong>现在就成功将本地网站部署到Github pages上啦，任何人都可以访问你的网站！后续你还可以更换域名、更换主题等等~</strong></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">nekkoya</p>
  <div class="site-description" itemprop="description">此博客正式搭建于2022年10月2日，欢迎光临~</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">nekkoya</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
